---
title: "p2dds"
output: html_document
date: "2025-01-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.

```{r, attr.warning=F}
# Instalar los paquetes necesarios si no están instalados
if (!require(httr)) install.packages("httr")
if (!require(XML)) install.packages("XML")

# Cargar las librerías
library(httr)
library(XML)
library(dplyr)

# Paso 1: Descargar la página web
url <- "https://www.mediawiki.org/wiki/MediaWiki"
response <- GET(url)

# Comprobar el estado de la respuesta
if (status_code(response) == 200) {
  cat("Página descargada correctamente.\n")
} else {
  stop("Error al descargar la página. Código de estado: ", status_code(response))
}

# Paso 2: Convertir HTML a formato XML
html_content <- content(response, as = "text", encoding = "UTF-8")
xml_content <- htmlParse(html_content, encoding = "UTF-8")
```

### 2. Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”).

```{r}
# Paso 3: Extraer el título de la página usando XPath
page_title <- xpathSApply(xml_content, "//title", xmlValue)

# Mostrar el título
cat("El título de la página es:", page_title, "\n")
```

### 3. Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como “a”), buscando el texto del enlace, así como la URL.

```{r}
# Paso 3: Extraer los enlaces (<a>) y sus atributos
# Extraer el texto del enlace
link_texts <- xpathSApply(xml_content, "//a", xmlValue)

# Extraer los URLs del atributo href
link_urls <- xpathSApply(xml_content, "//a/@href")

# Paso 4: Manejar posibles valores NULL en los resultados
link_texts[is.null(link_texts)] <- NA
link_urls[is.null(link_urls)] <- NA

# Paso 5: Combinar los textos y URLs en un data frame
links_df <- data.frame(
  Text = link_texts,
  URL = link_urls,
  stringsAsFactors = FALSE
)

# Mostrar una vista previa de los enlaces extraídos
print(head(links_df))

# Paso 6: (Opcional) Guardar los enlaces en un archivo CSV
#write.csv(links_df, "extracted_links.csv", row.names = FALSE)
#cat("Los enlaces extraídos se han guardado en 'extracted_links.csv'\n")
```

### 4. Generar una tabla con cada enlace encontrado, indicando el texto que acompaña el enlace, y el número de veces que aparece un enlace con ese mismo objetivo.

```{r}
# Paso 6: Contar ocurrencias de cada combinación de texto y enlace
link_summary <- as.data.frame(table(links_df$Text, links_df$URL))
colnames(link_summary) <- c("Text", "URL", "Count")

# Filtrar solo los enlaces con recuentos mayores a 0
link_summary <- link_summary[link_summary$Count > 0, ]

# Mostrar la tabla resultante
print(head(link_summary))

# Paso 7: (Opcional) Guardar la tabla en un archivo CSV
#write.csv(link_summary, "link_summary.csv", row.names = FALSE)
#cat("Resumen de enlaces guardado en 'link_summary.csv'\n")
```

### 5. Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de status HTTP al hacer una petición a esa URL).

```{r}
# Resolver URLs relativas y absolutas
base_url <- "https://www.mediawiki.org"
resolve_url <- function(link) {
  if (is.na(link)) return(NA)
  if (grepl("^http", link)) {
    return(link)  # URL absoluta
  } else if (grepl("^//", link)) {
    return(paste0("https:", link))  # Subdominio
  } else if (grepl("^/", link)) {
    return(paste0(base_url, link))  # URL relativa
  } else if (grepl("^#", link)) {
    return(url)  # Mismo documento, diferente altura
  } else {
    return(NA)  # No válido
  }
}

resolved_urls <- sapply(link_urls, resolve_url)

# Crear un data.frame con texto y URLs
links_df <- data.frame(
  Text = link_texts,
  URL = resolved_urls,
  stringsAsFactors = FALSE
)

# Contar repeticiones de cada enlace
link_summary <- count(links_df, Text, URL)
colnames(link_summary) <- c("Text", "URL", "Seen")

# Mostrar resultado
print(link_summary)

```
